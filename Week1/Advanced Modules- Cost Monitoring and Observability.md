---
tags:
  - cloud/12weeksworkshop
  - cloud/aws
---
This workshop is level 200-300. The audience is not expected to be familiar with AWS Cost Monitoring and Observability tools but it will help to have analytics knowledge because we will be building dashboards.

This workshop is composed of 2 sections.

**Section 1: Introduction to Cost Management**,In this section, we will walkthrough the foundational tools that need to be setup for efficient cost monitoring. We will showcase foundational topics like AWS Organizations, Cost Allocation Tags, Billing and various cost management tools. We will conclude the walkthrough with a recommendation tool for rightsizing, savings plans, and reserved instances.

**Section 2: Cost Intelligence Dashboards**, iDo you know how much you’re spending per hour on AWS Lambda? How much you are saving with Reserved Instances and Savings Plan? Does your team know how much their application costs to run on AWS? Visualizing and understanding your cost and usage data is critical for a good cloud financial management and accountability.

Cloud Financial Management (CFM) is the practice of bringing financial accountability to the variable spend model of cloud. CFM practitioners are pursuing business efficiency across all of their accounts by first visualizing their cost and usage, then setting goals, and finally driving accountability from their IT teams to meet or exceed these goals.
### **[Create IAM Users and Groups](#)**
- Search `IAM` in the search bar to enter the Identity and Access Management dashboard.
- Select **Users** from the options listed on the left side of the screen, and then select **Add Users**.
- Select the checkbox for **Provide user access to the AWS Management Console** and select **I want to create an IAM user**.
- Select **Custom Password**, and enter your password.
- Uncheck the checkbox for **Users must create a new password** , and select **Next**. _(This prevents the user from needing to create a new password before starting the workshop)_
![](https://i.imgur.com/i4DOztz.png)

User Groups allow for multiple users to share identical permissions when added to a user group with an assigned group policy.

On the left-hand menu select **User Groups**, and then select **Create Group**.
Enter `Sys_Admin_Group`, for the group name.
![](https://i.imgur.com/Dq3L2wO.png)

#### **[Enabling Billing Alarms and Cost Allocation Tags](#)**
<mark style="background: #BBFABBA6;">Billing Alarms monitor your AWS usage charges and recurring fees automatically, making it easier to track and manage your spending on AWS.</mark> You can set up billing alerts to receive email notifications when your charges reach a specified threshold.
A tag is a label that you or AWS assigns to an AWS resource. Each tag consists of a key and a value. For each resource, each tag key must be unique, and each tag key can have only one value.
_You can use tags to organize your resources, and cost allocation tags to track your AWS costs on a detailed level._ 
After you activate cost allocation tags, AWS uses the cost allocation tags to organize your resource costs on your cost allocation report, to make it easier for you to categorize and track your AWS costs.

[Enabling billing Alarms](#)
Begin by searching for `Billing` in the search bar, and select the **Billing Service**.
- Select **Billing Preferences** in the left-hand menu.
	Verify that alert preference are enabled. 
- Select **Cost Allocation Tags** on the left-hand menu.
There are two types of Cost Allocation Tags: 
	**User-Defined** (manual user-created tags) 
	**AWS-Created** (created automatically by AWS when resource is created).

![](https://i.imgur.com/PVWufHX.png)

Navigate to the **AWS generated cost allocation tags** tab, select **aws-createdBy**, and select **Activate**. (**Note:** _This enables tracking of resources by the tag generated by AWS during their creation._)


#### **[Setup AWS Organizations](#)**
AWS Organizations helps you centrally manage and govern your environment as you grow and scale your AWS resources. 
<mark style="background: #BBFABBA6;">Using AWS Organizations, you can create accounts and allocate resources, group accounts to organize your workflows, apply policies for governance, and simplify billing by using a single payment method for all of your accounts.</mark>
<mark style="background: #D2B3FFA6;">AWS Organizations is integrated with other AWS services so you can define central configurations, security mechanisms, audit requirements, and resource sharing across accounts in your organization. </mark>AWS Organizations is available to all AWS customers at no additional charge.
![](https://i.imgur.com/8RopEI2.png)


Search `Organizations` in the search bar at the top of the page.
Select the **Create an organization** button.
- [*] Organization Creation. Once the organization is generated, a verification email will be sent to the AWS account email on file. This email needs to be verified before an account can be added to the organization.
Select **Add an AWS Account**.
Select **Create a new account**. 
_(There are two ways to add an account to an AWS Organization, create a new account or add an existing account. 
If the root account has not been added to an Organization yet, the option to add it will also be available.)_

[NOTES](#)
_It is recommended that you do not enable trusted access with an AWS service on this page. Instead, use the service’s console to enable and disable trusted access with AWS Organizations. This allows the other service to perform any supporting tasks needed to enable or disable access with Organizations. For more information, see the documentation for the AWS service you want to use._

The **Services** tab allows a user to give trust access to services to be used across the organization.
**Note:** _We recommend that you do not enable trusted access with an AWS service on this page. Instead, use the service’s console to enable and disable trusted access with AWS Organizations. This allows the other service to perform any supporting tasks needed to enable or disable access with Organizations_
![](https://i.imgur.com/f3WTRh9.png)


The **Policies** tab allows a user to specify policies that need to be maintained across the organization.
![](https://i.imgur.com/U2bHePd.png)


The **Settings** tab provides a summary of the details of the organization, administrators delegated to manage the services or organization, and the ability to delete the organization.
![](https://i.imgur.com/otfljJW.png)

[Create Cloud Resources](#)
_Create EC2 Instance with the AWS Console(Option 1)_
The next step in the process is to create a resource to monitor billing and usage details.
- Navigate to the EC2 Console
- Select **Launch instance**
- Enter **Instance Name**: `CostOpInstance`
	- **Instance Image**: Amazon Linux 2 AMI (Default)
	- **Instance Type**: t2.micro (Default)
	- **Number of Instances**: 2
- Select **Create new key pair**.
	- Enter **Key pair name**: `CostOpKey`
	-  - Verify RSA is selected for **Key pair type**.
	- For the private key file format: if using Windows - select **.ppk**, if using MacOs/Linux - select **.pem**.
- -Select **Allow HTTPS traffic from the internet**
- Select **Allow HTTP traffic from the internet** to allow access to the EC2 webpage.
- Select the **Launch Instance** button to launch the EC2 instance.
- Next step: **"Setup AWS Cost Management Tools"** by clicking the **Next** button at the bottom of the page, or try the optional method to launch an EC2 instance with AWS CloudFormation below.

_Create EC2 Instance with AWS CloudFormation(Option 2)_
Navigate to the Cloud formation service page upload the template (FinalTemplate.yml) and select **Create Stack**
- Select the **Choose File** button
- Select **final_template.yml** file that was downloaded in the first step
- Select **Next**.
- **Stack Name**: `MyStack`
- **Instance Type**: _T3.micro_
- Select **Next**.
- Add a tag with the following values:
    - **Key**: _Name_
    - **Value**: _CostOp_
    - Select **Next**
    - Select **Submit** to deploy the EC2 instance.

#### Setup AWS Cost Management Tools
The Cost Management Console provides useful tools to help gather information related to cost and usage, analyze cost drivers and usage trends, take actions to budget spending, identify cost anomalies, and purchase savings plans.
[Setup AWS Cost Explorer](#)
setup the Cost Explorer tool that is used for generating cost reports and performing the cost analysis for resource consumption.
[Setup AWS Budgets](#)
setup the AWS Budgets tool that is used to monitor spending and create notifications when amounts go past the set pricing limit.
[Setup AWS Anomaly Detection](#)
Setup the AWS Anomaly Detection tool that leverages advanced machine learning technologies to identify anomalous spend and root causes, to quickly take action. 
[Optimize Resources](#)
Setup cost reporting, cost saving opportunities by rightsizing EC2 resources and provide sign up for savings plans or reserved instances. 

**[Setup AWS Cost Explorer](#)**
AWS Cost Explorer has an easy-to-use interface that lets a user visualize, understand, and manage AWS costs and usage over time. This can be done quickly by creating custom reports that analyze cost and usage data. Data analysis can also be performed at a high level (for example, total costs and usage across all accounts) or dive deeper into cost and usage data to identify trends, pinpoint cost drivers, and detect anomaly .
On you AWS console search for **AWS Cost Explorer**
- Select **AWS Cost Management** page
- Select **Preferences** from the menu on the left side of the AWS Cost Management Console.
![](https://i.imgur.com/n9ZcMyp.png)
Select **Cost Explorer** from the menu on the left and review the current Cost and Usage Breakdown. **Note**: _This report can be downloaded as a CSV file by clicking the_ **Download as CSV** _button_.
![](https://i.imgur.com/MnCtP1C.png)
[Create Cost Reports](#)
Select **Reports** from the menu on the left side of the AWS Cost Management Console. **Note**: _There are default templates that can be used based on cost and usage, utilization, or coverage._
Select **Cost and usage** and select **Create Report**.
![](https://i.imgur.com/lue5dJo.png)

Modify the type of graph by selecting 1 of the 3 icons, or save the report by clicking the **Save to report library** button
![](https://i.imgur.com/xu2WKnr.png)

Move to the next step: **"Setup AWS Budgets"** by selecting the **Next** button at the bottom of the screen.

[AWS Budgets](#)
Navigate to the **AWS Budgets Service Page**
Select **Create a budget**.
Keep the default values, enter the email addresses that need to receive notifications, and select **Create Budget**.
After the budget has been created, you will be taken to the budgets dashboard where it shows a summary of both budgeted and historical AWS spend, and any alarms that have exceeded the specified thresholds.


![](https://i.imgur.com/BmLGY4R.png)

Move to the next step: **"Setup AWS Cost Anomaly Detection"** by selecting the **Next** button at the bottom of the page.

[AWS Cost Anomaly Detection](#)
Navigate to the **AWS Cost Anomaly Detection** page by going back to the Cost Explorer page.
On the overview screen, select the **Cost monitors** tab at the bottom, and then select the **Create monitor** button.
![](https://i.imgur.com/bPrqbaf.png)
![](https://i.imgur.com/TvWcv2z.png)
On the **Configure alert subscriptions** page, Select **Create a new subscription**.
![](https://i.imgur.com/rnMvuhJ.png)
The next page is a confirmation of the successful creation and summary of the monitor.
![](https://i.imgur.com/1LKK3ql.png)


[Optimize Resources](#)
There are several ways to optimize resources on AWS. 
Three recommendations that are available with the AWS Cost Management Console involve **rightsizing, savings plans, and reserved instances.**
[Rightsizing recommendations](#)
	Rightsizing recommendations review the historical EC2 usage to identify opportunities for greater cost and usage efficiency. By default, recommendations consider usage for the past 14 days. 
	If Compute Optimizer’s enhanced infrastructure metrics paid feature is activated for a resource, the recommendations for that resource will consider the past 3 months.
	Select **Rightsizing recommendations** from the menu on the left of the AWS Cost Management Console page.
[Savings Plan Recommendations](#)
	AWS provides customized Savings Plans recommendations based on past usage to save costs. 
	Based on the usage, a quote is calculated for what your bill could have been if you purchased an additional Savings Plan commitment for that time period. 
	The commitment value that is estimated to result in the largest monthly savings is identified and recommended.
	Select **Recommendations** under the **Savings Plans** section from the menu on the left.
	![](https://i.imgur.com/kfXYYPr.png)
	Select **Compute Saving Plan** to view the recommended plans to reduce compute costs. 
	**Note**: _Compute Savings apply to EC2 instance usage, AWS Fargate, and AWS Lambda service usage, regardless of region, instance family, size, tenancy, and operating system._
	![](https://i.imgur.com/C3lO91G.png)
[Reserved Instance Recommendations](#)
AWS Cost Explorer provides Reserved Instances (RI) purchase recommendations based on Amazon EC2, Amazon RDS, Amazon Redshift, Amazon ElastiCache, and Amazon Elasticsearch usage.

Select **Recommendations** under the **Reservations** section from the menu on the left. This will open up a view to show the recommended Reserved Instances. **Note** _To tune the results being generated, select unique values in the_ **Recommendation parameters** _section._
![](https://i.imgur.com/E2oXgGZ.png)


### Cost Intelligence Dashboards
#### Setting up Cost and Usage Reports
In this section, you will learn how to set up cost and usage reports and have the reports delivered to an S3 bucket.
#### AWS Glue
In this section, you will learn how to use AWS Glue to define a schema for our cost and usage data that is being delivered to a S3 Bucket. Defining the schema will allows us to use SQL queries on the cost and usage data.
#### Amazon Athena
In this section, you will use Amazon Athena to create views on the cost and usage data. Creating views of the data will help us to aggregate the information we need to create visualizations.
#### Amazon QuickSight
In this section, you will setup up Amazon QuickSight and integrate with Amazon Athena as a data source.
#### CloudShell
In this section, you will have a chance to deploy the Cost Intelligence Dashboard on QuickSight using CloudShell.

[Cost and Usage Reports](#)
<mark style="background: #BBFABBA6;">The AWS Cost & Usage Report (CUR) contains the most comprehensive set of AWS cost and usage data available, including additional metadata about AWS services, pricing, Reserved Instances, and Savings Plans. </mark>
The CUR itemizes usage at the account or Organization level by product code, usage type and operation. These costs can be further organized by enabling Cost Allocation tags and Cost Categories. 
The AWS Cost & Usage Report is available at an hourly, daily, or monthly level of granularity.
- Go to Billing Service
- Click on Cost and Usage Reports.
- Click on **Create Report**, Name the report `CostReportDemo`. Check the box for _Include Resources IDs_. Click _Next_
- Click on **Configure** to create a S3 bucket. Name the bucket `demo-curs-[Account-ID]` 
- Replace the [Account-ID] with your AWS account ID found in the top right. Click the check box for "The following default policy will be applied to your bucket".
- Click **Save**
- ![](https://i.imgur.com/k5Swy55.png)
![](https://i.imgur.com/HiBqI8y.png)

Enter **Report path prefix** as `/curs`
Change Report Versioning to **Overwrite existing report**
Enable Data Integration for **Amazon Athena**
Click **Next**
Scroll down and click **Create Report**
![](https://i.imgur.com/RTaIRhs.png)

<mark style="background: #D2B3FFA6;">In the next 24 hours, your first report will be delivered to the Amazon S3 bucket you configured during report creation. 
Since we don't have the time or historic AWS spend on this account we have provided you fabricated data for completing the demo.</mark>

[Setup Fabricated data](#)
- Go to S3 service. Create a bucket. Name the bucket `demo-costso-[Account-ID]` Replace the [Account-ID] with your AWS account ID found in the top right. 
- Leave everything default. Click **Create bucket**
- Go into the newly created bucket. Click on "Create folder". Name the folder `customer_all`. Click on **Create folder**
- Download the 12 month sample cost and usage report data. **SampleCURS**
- Go into the _customer_all_ folder. Drag and drop the sample CUR file.
- Click on **Upload**
- Click on close after upload finishes.
- <mark style="background: #FFF3A3A6;">Now we will use this sample parquet file as our raw dataset to build out our dashboard. </mark>

[AWS Glue](#)
<mark style="background: #BBFABBA6;">AWS Glue is a serverless data integration service that makes it easy for analytics users to discover, prepare, move, and integrate data from multiple sources.</mark> 
You can use it for analytics, machine learning, and application development. It also includes additional productivity and data ops tooling for authoring, running jobs, and implementing business workflows.
The <mark style="background: #BBFABBA6;">AWS Glue Data Catalog contains references to data that is used as sources and targets of your extract, transform, and load (ETL) jobs in AWS Glue. </mark>
The AWS Glue Data Catalog is an index to the location, schema, and runtime metrics of your data. Information in the Data Catalog is stored as metadata tables, where each table specifies a single data store. Typically, you run a crawler to take inventory of the data in your data stores, but there are other ways to add metadata tables into your Data Catalog.

In this lab we are going to <mark style="background: #BBFABBA6;">use a crawler to populate the AWS Glue Data Catalog with a table. The crawler will crawl through the S3 bucket location for our fabricated cost and usage dataset. Upon completion, the crawler will create a table in our AWS Glue Data Catalog.</mark>

[Create Glue Data Catalog Table](#)
Go to **AWS Glue**
Click on **Data Catalog** in the side menu. Click on **Crawlers** and choose **Create crawler** 
![](https://i.imgur.com/s2D63EQ.png)
Enter `Crawler-Demo` for the name. Click **Next**.
Choose **Add a data source**
Choose **Browse** for S3 path and select the S3 bucket and then select the **customer_all** folder. 
![](https://i.imgur.com/jwUWY6H.png)

![](https://i.imgur.com/j3XpoJx.png)

Leave the rest default and choose **Add an S3 data source**
![](https://i.imgur.com/6koOhln.png)
Choose **Next**
Choose **Create new IAM Role**. Name it `AWSGlueServiceRole-Demo` and click **Create**. 
![](https://i.imgur.com/PBg3Pup.png)

Choose **Next**
Choose **Add database** Enter `customer_cur_data` for the name keep other values default. Click **Create Database**.
And choose the **customer_cur_data** database. Click on **Advanced options**.
![](https://i.imgur.com/wGUTwYW.png)
Enable **Create a single schema for each S3 path**
Enable **Update all new and existing partitions with metadata from the table**
Choose **Delete tables and partitions from the data catalog**.
Choose **Next**
![](https://i.imgur.com/q3e6kiE.png)
Choose **Create crawler**
Choose **Run Crawler**
This will take about 1 minute for the crawler to finish the job and the status to be completed.
<mark style="background: #BBFABBA6;">The crawler was able to go through our data and create a table and schema. We will now integrate this schema with Amazon Athena so we can write SQL queries on the S3 data.</mark>
![](https://i.imgur.com/TkttlCW.png)

![](https://i.imgur.com/k6ONii5.png)


#### **[Define Schema](#)**
**[Amazon Athena](#)**
<mark style="background: #BBFABBA6;">Athena is an interactive analytics service that makes it easier to analyze data in Amazon Simple Storage Service (S3) using Python or standard SQL. Athena is serverless, so there is no infrastructure to set up or manage, and you can start analyzing data immediately. 
You don’t even need to load your data into Athena; it works directly with data stored in Amazon S3.</mark>

<mark style="background: #D2B3FFA6;">Athena for SQL uses a managed AWS Glue Data Catalog to store information and schemas about the databases and tables that you create for your data stored in S3.</mark>

In this section, <mark style="background: #BBFABBA6;">we will use the Athena to create views by leveraging Glue Data Catalog schema we created for the data that is stored in S3.</mark> 
<mark style="background: #ABF7F7A6;">In the next step, the data source for the dashboard will be the Athena views of the fabricated Cost and Usage Report (CUR).</mark>

[Create Views](#)
- Go to **Amazon Athena**
- Click on **Launch query editor** on the right side of the screen
![](https://i.imgur.com/MPUDmmJ.png)
- Verify the Glue database we created and the table created by the crawler are showing up.
![](https://i.imgur.com/bpRoUvw.png)
Choose **Settings**
Click on **Manage**
Choose **Browse S3**. Select the **demo-costso-[Acccount-ID]** bucket and click **Choose**.
Click **Save**
![](https://i.imgur.com/YHHuRsw.png)

Choose **Workgroups** on the left menu
Click on the **primary** workgroup. Click on **Edit** at the top right
Scroll down & Expand on **Query result configuration**. Choose **Browse S3**. Select the **demo-costso-[Acccount-ID]** bucket and click **Choose**.
![](https://i.imgur.com/AwsYBaX.png)

Scroll down and choose **Save changes**
Click on **Query Editor** on the left hand menu
Copy the SQL query to create Account map view and paste it into the query editor. Choose run
**Account map view**
```SQL
 CREATE OR REPLACE VIEW account_map AS
 SELECT DISTINCT
     "line_item_usage_account_id" "account_id", "line_item_usage_account_id" "account_name"
 FROM
     customer_cur_data.customer_all;
```
![](https://i.imgur.com/8KQW6Du.png)

Copy the SQL query to create Summary view and paste it into the query editor. Choose **Run**
**Summary view**
```SQL
  CREATE OR REPLACE VIEW summary_view AS
  SELECT
  "year"
  , "month"
  ,  "bill_billing_period_start_date" "billing_period"
  , "date_trunc"('day', "line_item_usage_start_date") "usage_date"
  , "bill_payer_account_id" "payer_account_id"
  , "line_item_usage_account_id" "linked_account_id"
  , "bill_invoice_id" "invoice_id"
  , "line_item_line_item_type" "charge_type"
  , CASE 
      WHEN ("line_item_line_item_type" = 'DiscountedUsage') THEN 'Running_Usage' 
      WHEN ("line_item_line_item_type" = 'SavingsPlanCoveredUsage') THEN 'Running_Usage' 
      WHEN ("line_item_line_item_type" = 'Usage') THEN 'Running_Usage' ELSE 'non_usage' END "charge_category"
  , CASE 
      WHEN ("savings_plan_savings_plan_a_r_n" <> '') THEN 'SavingsPlan' 
      WHEN ("reservation_reservation_a_r_n" <> '') THEN 'Reserved' 
      WHEN ("line_item_usage_type" LIKE '%Spot%') THEN 'Spot' 
      ELSE 'OnDemand' END "purchase_option"
  , CASE 
      WHEN ("savings_plan_savings_plan_a_r_n" <> '') THEN "savings_plan_savings_plan_a_r_n" 
      WHEN ("reservation_reservation_a_r_n" <> '') THEN "reservation_reservation_a_r_n" ELSE '' END "ri_sp_arn"
  , "line_item_product_code" "product_code"
  , "product_product_name" "product_name"
  , CASE 
      WHEN ("bill_billing_entity" = 'AWS Marketplace' AND "line_item_line_item_type" NOT LIKE '%Discount%') THEN "Product_Product_Name" 
      WHEN ("product_servicecode" = '') THEN "line_item_product_code" ELSE "product_servicecode" END "service"
  , "product_product_family" "product_family"
  , "line_item_usage_type" "usage_type"
  , "line_item_operation" "operation"
  , "line_item_line_item_description" "item_description"
  , "line_item_availability_zone" "availability_zone"
  , "product_region" "region"
  , CASE
      WHEN (("line_item_usage_type" LIKE '%Spot%') AND ("line_item_product_code" = 'AmazonEC2') AND ("line_item_line_item_type" = 'Usage')) THEN "split_part"("line_item_line_item_description", '.', 1) ELSE "product_instance_type_family" END "instance_type_family"
  , CASE 
  WHEN (("line_item_usage_type" LIKE '%Spot%') AND ("line_item_product_code" = 'AmazonEC2') AND ("line_item_line_item_type" = 'Usage')) THEN "split_part"("line_item_line_item_description", ' ', 1) ELSE "product_instance_type" END "instance_type"
  , CASE 
  WHEN (("line_item_usage_type" LIKE '%Spot%') AND ("line_item_product_code" = 'AmazonEC2') AND ("line_item_line_item_type" = 'Usage')) THEN "split_part"("split_part"("line_item_line_item_description", ' ', 2), '/', 1) ELSE "product_operating_system" END "platform" 
  , "product_tenancy" "tenancy"
  , "product_physical_processor" "processor"
  , "product_processor_features" "processor_features"
  , "product_database_engine" "database_engine"
  , "product_group" "product_group"
  , "product_from_location" "product_from_location"
  , "product_to_location" "product_to_location"
  , "product_current_generation" "current_generation"
  , "line_item_legal_entity" "legal_entity"
  , "bill_billing_entity" "billing_entity"
  , "pricing_unit" "pricing_unit"
  , approx_distinct("Line_item_resource_id") "resource_id_count"
  , sum(CASE 
  WHEN ("line_item_line_item_type" = 'SavingsPlanCoveredUsage') THEN "line_item_usage_amount" 
  WHEN ("line_item_line_item_type" = 'DiscountedUsage') THEN "line_item_usage_amount" 
  WHEN ("line_item_line_item_type" = 'Usage') THEN "line_item_usage_amount" ELSE 0 END) "usage_quantity"
  , sum ("line_item_unblended_cost") "unblended_cost"
  , sum(CASE
      WHEN ("line_item_line_item_type" = 'SavingsPlanCoveredUsage') THEN "savings_plan_savings_plan_effective_cost" 
      WHEN ("line_item_line_item_type" = 'SavingsPlanRecurringFee') THEN ("savings_plan_total_commitment_to_date" - "savings_plan_used_commitment") 
      WHEN ("line_item_line_item_type" = 'SavingsPlanNegation') THEN 0
      WHEN ("line_item_line_item_type" = 'SavingsPlanUpfrontFee') THEN 0
      WHEN ("line_item_line_item_type" = 'DiscountedUsage') THEN "reservation_effective_cost"  
      WHEN ("line_item_line_item_type" = 'RIFee') THEN ("reservation_unused_amortized_upfront_fee_for_billing_period" + "reservation_unused_recurring_fee")
      WHEN (("line_item_line_item_type" = 'Fee') AND ("reservation_reservation_a_r_n" <> '')) THEN 0 ELSE "line_item_unblended_cost" END) "amortized_cost"
  , sum(CASE
      WHEN ("line_item_line_item_type" = 'SavingsPlanRecurringFee') THEN (-"savings_plan_amortized_upfront_commitment_for_billing_period") 
      WHEN ("line_item_line_item_type" = 'RIFee') THEN (-"reservation_amortized_upfront_fee_for_billing_period") ELSE 0 END) "ri_sp_trueup"
  , sum(CASE
      WHEN ("line_item_line_item_type" = 'SavingsPlanUpfrontFee') THEN "line_item_unblended_cost"
      WHEN (("line_item_line_item_type" = 'Fee') AND ("reservation_reservation_a_r_n" <> '')) THEN "line_item_unblended_cost"ELSE 0 END) "ri_sp_upfront_fees"
  , sum(CASE
      WHEN ("line_item_line_item_type" <> 'SavingsPlanNegation') THEN "pricing_public_on_demand_cost" ELSE 0 END) "public_cost" 
  FROM
  customer_cur_data.customer_all
  WHERE (("bill_billing_period_start_date" >= ("date_trunc"('month', current_timestamp) - INTERVAL  '7' MONTH)) AND (CAST("concat"("year", '-', "month", '-01') AS date) >= ("date_trunc"('month', current_date) - INTERVAL  '7' MONTH)))

  GROUP BY 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34


```
![](https://i.imgur.com/s8SixVp.png)

Copy the SQL query to create EC2 Running Costs view and paste it into the query editor. Choose **Run**
**EC2 running costs view**
```sql
 CREATE OR REPLACE VIEW "ec2_running_cost" AS 
 SELECT DISTINCT
 "year"
 , "month"
 , "bill_billing_period_start_date" "billing_period"
 ,  "date_trunc"('hour', "line_item_usage_start_date") "usage_date"
 , "bill_payer_account_id" "payer_account_id"
 , "line_item_usage_account_id" "linked_account_id"
 , (CASE WHEN ("savings_plan_savings_plan_a_r_n" <> '') THEN 'SavingsPlan' WHEN ("reservation_reservation_a_r_n" <> '') THEN 'Reserved' WHEN ("line_item_usage_type" LIKE '%Spot%') THEN 'Spot' ELSE 'OnDemand' END) "purchase_option"
 , "sum"(CASE
     WHEN "line_item_line_item_type" = 'SavingsPlanCoveredUsage' THEN "savings_plan_savings_plan_effective_cost"
     WHEN "line_item_line_item_type" = 'DiscountedUsage' THEN "reservation_effective_cost"
     WHEN "line_item_line_item_type" = 'Usage' THEN "line_item_unblended_cost"
     ELSE 0 END) "amortized_cost"
 , "round"("sum"("line_item_usage_amount"), 2) "usage_quantity"
 FROM
 customer_cur_data.customer_all
 WHERE (((((("bill_billing_period_start_date" >= ("date_trunc"('month', current_timestamp) - INTERVAL  '1' MONTH)) AND ("line_item_product_code" = 'AmazonEC2')) AND ("product_servicecode" <> 'AWSDataTransfer')) AND ("line_item_operation" LIKE '%RunInstances%')) AND ("line_item_usage_type" NOT LIKE '%DataXfer%')) AND ((("line_item_line_item_type" = 'Usage') OR ("line_item_line_item_type" = 'SavingsPlanCoveredUsage')) OR ("line_item_line_item_type" = 'DiscountedUsage')))
 GROUP BY 1, 2, 3, 4,5,6,7

```
![](https://i.imgur.com/NDTA3uE.png)


Copy the SQL query to create Compute Savings Plan Eligible Spend view and paste it into the query editor. Choose run
**Compute savings plan eligible spend view**
```SQL
 CREATE OR REPLACE VIEW "compute_savings_plan_eligible_spend" AS 
 SELECT DISTINCT
 "year"
 , "month"
 , "bill_payer_account_id" "payer_account_id"
 , "line_item_usage_account_id" "linked_account_id"
 , "bill_billing_period_start_date" "billing_period"
 , "date_trunc"('hour', "line_item_usage_start_date") "usage_date"
 , "sum"(CASE
     WHEN (((("line_item_line_item_type" = 'Usage') AND (NOT ("line_item_usage_type" LIKE '%Spot%'))) AND ("product_servicecode" <> 'AWSDataTransfer')) AND ("line_item_usage_type" NOT LIKE '%DataXfer%')) THEN
         CASE
             WHEN (("line_item_product_code" = 'AmazonEC2') AND ("line_item_operation" LIKE '%RunInstances%')) THEN "line_item_unblended_cost"
             WHEN (("line_item_product_code" = 'AWSLambda') AND ("line_item_usage_type" LIKE '%Lambda-Provisioned-GB-Second%')) THEN "line_item_unblended_cost"
             WHEN (("line_item_product_code" = 'AWSLambda') AND ("line_item_usage_type" LIKE '%Lambda-GB-Second%')) THEN "line_item_unblended_cost"
             WHEN (("line_item_product_code" = 'AWSLambda') AND ("line_item_usage_type" LIKE '%Lambda-Provisioned-Concurrency%')) THEN "line_item_unblended_cost"
             WHEN ("line_item_usage_type" LIKE '%Fargate%') THEN "line_item_unblended_cost"
             ELSE 0
         END
     ELSE 0 
     END) "unblended_cost"
 FROM
 customer_cur_data.customer_all
 WHERE (("bill_billing_period_start_date" >= ("date_trunc"('month', current_timestamp) - INTERVAL  '1' MONTH)) AND ("line_item_usage_start_date" < ("date_trunc"('day', current_timestamp) - INTERVAL  '1' DAY)) AND ((("line_item_product_code" = 'AmazonEC2') AND ("line_item_operation" LIKE '%RunInstances%')) OR (("line_item_product_code" = 'AWSLambda') AND ("line_item_usage_type" LIKE '%Lambda-Provisioned-GB-Second%')) OR (("line_item_product_code" = 'AWSLambda') AND ("line_item_usage_type" LIKE '%Lambda-GB-Second%')) OR (("line_item_product_code" = 'AWSLambda') AND ("line_item_usage_type" LIKE '%Lambda-Provisioned-Concurrency%')) OR ("line_item_usage_type" LIKE '%Fargate%')) AND ("line_item_line_item_type" = 'Usage') AND ("line_item_usage_type" NOT LIKE '%Spot%') AND ("product_servicecode" <> 'AWSDataTransfer') AND ("line_item_usage_type" NOT LIKE '%DataXfer%')) 
 GROUP BY 1, 2, 3, 4,5,6

```
![](https://i.imgur.com/S893Loo.png)


Copy the SQL query to create S3 view and paste it into the query editor. Choose "Run"
**S3 view**
```sql
 CREATE OR REPLACE VIEW "s3_view" AS 
 SELECT DISTINCT
 "year"
 , "month"
 , "bill_billing_period_start_date" "billing_period"
 , "date_trunc"('day', "line_item_usage_start_date") "usage_date"
 , "bill_payer_account_id" "payer_account_id"
 , "line_item_usage_account_id" "linked_account_id"
 , "line_item_resource_id" "resource_id"
 , "line_item_product_code" "product_code"
 , "line_item_operation" "operation"
 , "product_region" "region"
 , "line_item_line_item_type" "charge_type"
 , "pricing_unit" "pricing_unit"
 , "sum"(CASE
     WHEN ("line_item_line_item_type" = 'Usage') THEN "line_item_usage_amount"
     ELSE 0
     END) "usage_quantity"
 , "sum"("line_item_unblended_cost") "unblended_cost"
 , "sum"("pricing_public_on_demand_cost") "public_cost"
 FROM 
 customer_cur_data.customer_all
 WHERE (((("bill_billing_period_start_date" >= ("date_trunc"('month', current_timestamp) - INTERVAL  '3' MONTH)) AND ("line_item_usage_start_date" < ("date_trunc"('day', current_timestamp) - INTERVAL  '1' DAY))) AND ("line_item_operation" LIKE '%Storage%')) AND (("line_item_product_code" LIKE '%AmazonGlacier%') OR ("line_item_product_code" LIKE '%AmazonS3%')))
 GROUP BY 1, 2, 3, 4, 5, 6, 7, 8, 9, 10,11,12

```
![](https://i.imgur.com/ZuU1Ran.png)


Copy the SQL query to create RI SP Mapping view and paste it into the query editor. Choose run
**RI SP Mapping view**
```SQL
  CREATE OR REPLACE VIEW "ri_sp_mapping" AS 
   SELECT DISTINCT
   "bill_billing_period_start_date" "billing_period_mapping"
  , "bill_payer_account_id" "payer_account_id_mapping"
  , CASE 
     WHEN ("savings_plan_savings_plan_a_r_n" <> '') THEN "savings_plan_savings_plan_a_r_n" 
     WHEN ("reservation_reservation_a_r_n" <> '') THEN "reservation_reservation_a_r_n"ELSE '' END "ri_sp_arn_mapping"
  , CASE 
     WHEN ("savings_plan_savings_plan_a_r_n" <> '') THEN CAST(from_iso8601_timestamp("savings_plan_end_time") AS timestamp)
     WHEN ("reservation_reservation_a_r_n" <> '' AND "reservation_end_time" <> '') THEN CAST(from_iso8601_timestamp("reservation_end_time") AS timestamp) ELSE NULL END "ri_sp_end_date"
  , CASE 
     WHEN ("savings_plan_savings_plan_a_r_n" <> '') THEN "savings_plan_purchase_term" 
  WHEN ("reservation_reservation_a_r_n" <> '') THEN "pricing_lease_contract_length"ELSE '' END "ri_sp_term"
  , CASE 
     WHEN ("savings_plan_savings_plan_a_r_n" <> '') THEN "savings_plan_offering_type" 
     WHEN ("reservation_reservation_a_r_n" <> '') THEN "pricing_offering_class" ELSE '' END "ri_sp_offering"
  , CASE 
     WHEN ("savings_plan_savings_plan_a_r_n" <> '') THEN "savings_plan_payment_option" 
     WHEN ("reservation_reservation_a_r_n" <> '') THEN "pricing_purchase_option"	ELSE '' END "ri_sp_payment"			
     FROM
       customer_cur_data.customer_all
     WHERE (("line_item_line_item_type" = 'RIFee') OR ("line_item_line_item_type" = 'SavingsPlanRecurringFee'))

```
![](https://i.imgur.com/AVt4Grd.png)


<mark style="background: #FF5582A6;">You should now have 6 views showing up in the bottom left</mark>
![](https://i.imgur.com/E6Q2ief.png)


<mark style="background: #FFF3A3A6;">Now we have all the views we need to create the Cost Intelligence Dashboards. These views are the main datasets for your dashboard page</mark>


#### QuickSight
**[Amazon QuickSight](#)**

[Creating Amazon Athena Data Source on QuickSight](#)
Go to **Amazon QuickSight**
Choose **Sign up for QuickSight**
Leave default and choose **Continue**
Enter **QuickSight account name** and Enter your **Notification Email Address**. You won't be charged.
Select the check box on **Amazon S3** for **Allow access and autodiscovery for these resources**. Give permission to the appropriate bucket. Confirm with the picture below and click **Finish**
Click **Finish**
It will take about 1 minute for QuickSight to create your account. Click on **Go to Amazon QuickSight** when done.
Click on **Datasets**
Click on **New dataset** button at the top right    
Choose **Athena**
Enter `COSTSO` for the name. Click on **Create data source**.
We are done creating the data source. Close out of the screen.
![](https://i.imgur.com/8NDrESP.png)
Click on the person icon at the very top right.
![](https://i.imgur.com/4B4TnZy.png)
Click on **Manage QuickSight**
Click on **SPICE capacity** in the left menu
Click on **Purchase more capacity** button at the top right.
Enter 60 into the field and click **Purchase SPICE capacity**
![](https://i.imgur.com/1n6XC5U.png)

Click on the **QuickSight** icon at the very top left to go back to the QuickSight home page.

<mark style="background: #CACFD9A6;">We have everything setup and all that is left to do is deploy the dashboards. Have your QuickSight tab open and open another tab to access the AWS console.</mark>


#### Deploying the Dashboards
**AWS CloudShell**

[Deploying Dashboards](#)
Go to **CloudShell**
Download the **STAMCostOpsSOW.zip** file
In CloudShell, click on **Actions** and choose **Upload a file**.
![](https://i.imgur.com/AxyR8Qk.png)

Upload the **STAMCostOpsSOW.zip** file you downloaded and click **Upload**
![](https://i.imgur.com/NUxUXCS.png)![](https://i.imgur.com/kYkVu8H.png)
 

Once uploaded run the commands below
```bash
unzip STAMCostOpsSOW.zip

cd STAMCostOpsSOW

pip3 install -U .

cid-cmd deploy --recursive yes  -vvv

```

You might get a warning about copy pasting into CloudShell. Double check you are pasting the right commands and click **Paste**.
Use the arrow keys and press enter on the **Cost Intelligence Dashboard**
![](https://i.imgur.com/L7vNRa1.png)

- When you are prompted to share the dashboard with everyone in the account choose **Yes** by pressing enter.
- Run the `cid-cmd deploy --recursive yes -vvv` again.
- ![](https://i.imgur.com/t6NsRW4.png)
- Use the arrow keys and press enter on the **CUDOS Dashboard**
- ![](https://i.imgur.com/vhOHh3X.png)

When you are prompted to share the dashboard with everyone in the account choose **Yes** by pressing enter.  
Go back to **QuickSight**
Choose **Dashboards** on the left menu
Explore both of the Dashboards by clicking on each one.
![](https://i.imgur.com/gE577jh.png)
Since this is fabricated data some of it might not make sense. Feel free to go through each of the tabs in each dashboard to go through the visuals.

**[Cloud Intelligence Dashboards Clean Up](#)**
[QuickSight clean up](#)
1. Go to QuickSight and click on Dashboards
2. Delete all the Dashboards created
3. Click on the Person Icon at the top and right and choose **Manage QuickSight**
4. Choose **SPICE capacity** on the left menu
5. Choose **Release unsused purchase capacity**
6. Select the **Release all** option and choose **Release SPICE capacity**![](https://i.imgur.com/a7rBzMO.png)

7. Click on **Account settings** in the left menu
8. To get rid of the whole QuickSight account under **Account termination** click on **Manage**
9. Follow the prompt and terminate the account
![](https://i.imgur.com/VO1rK9Y.png)


[CloudShell Clean Up](#)
- Go to Cloud Shell
- Once the shell loads, run the commands below
```bash
cd /home/cloudshell-user

rm -rf STAMCostOpsSOW

rm -rf STAMCostOpsSOW.zip

```
[Glue Clean Up](#)
- Go to AWS Glue
- Got to Databases, Choose _customer_cur_data_ and click on "Delete"
- This will also clean up any Athena views we created
- Go to **Crawlers**. Select **Crawler-Demo**. Select **Actions**. Choose **Delete crawler**. Confirm **Delete**

[CURS Clean UP](#)
- Got to Billing Service
- Click on cost and usage reports
- Select **CostReportDemo**. Select **Actions**. Click **Delete**. Confirm **Delete**

[S3 Clean up](#)
- Go to S3 service
- You will find that 3 buckets were created for this workshop. demo-curs-[Account-ID], demo-costso-[Account-ID], aws-athena-query-results-us-east-1-[Account-ID]. Select each bucket and click **Empty**
You will need to confirm you want to empty by typing in **permanently delete** and choosing **Empty** for each bucket.
After you emptied all the buckets, you can choose each bucket and click on **Delete**. You will need to confirm the delete by typing in the bucket name and choosing **Delete**.